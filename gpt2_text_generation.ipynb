{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvaujDzapjuzE6S++uG8jl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sbowma15/gpt2_text_generation/blob/main/gpt2_text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "id": "zuOC2l3r6LeG",
        "outputId": "97b6ce4c-bbdd-4295-cdca-4ff9291c6590"
      },
      "source": [
        "!pip -m install pandas matplotlib numpy/\n",
        "nltk seaborn sklearn gensim pyldavis\n",
        "wordcloud textblob spacy textstat\n",
        "\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"cpierse/gpt2_film_scripts\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"cpierse/gpt2_film_scripts\")\n",
        "\n",
        "# Function to first select topN tokens from the probability list and then based on the N word distribution\n",
        "# select random token ID\n",
        "def choose_from_top(probs, n=5):\n",
        "    ind = np.argpartition(probs, -n)[-n:]\n",
        "    top_prob = probs[ind]\n",
        "    top_prob = top_prob / np.sum(top_prob) # Normalize\n",
        "    choice = np.random.choice(n, 1, p = top_prob)\n",
        "    token_id = ind[choice][0]\n",
        "    return int(token_id)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "\n",
        "    for i in range(250):\n",
        "        outputs = model(cur_ids, labels=cur_ids)\n",
        "        loss, logits = outputs[:2]\n",
        "        softmax_logits = torch.softmax(logits[0,-1], dim=0) #Take the first(only one) batch and the last predicted embedding\n",
        "        next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=5) #Randomly(from the given probability distribution) choose the next word from the top n words\n",
        "        cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim = 1) # Add the last word\n",
        "\n",
        "    output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
        "    output_text = tokenizer.decode(output_list)\n",
        "#print(output_text)\n",
        "\n",
        "decoded_output = []\n",
        "for sample in output_list:\n",
        "        decoded_output.append(tokenizer.decode(\n",
        "            sample, skip_special_tokens=True))\n",
        "print(decoded_output[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-736bd0260e81>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    nltk seaborn sklearn gensim pyldavis/\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}